{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce654aa3",
   "metadata": {},
   "source": [
    "# MindCV简介\n",
    "\n",
    "MindCV是一个基于MindSpore的计算机视觉库，集成了许多经典的或者最前沿的模型，例如ResNet和SwinTransformer；支持一些最新的数据增强方法，例如AutoAugment；也支持一些主流的图像数据集，例如CIFAR10。\n",
    "\n",
    "这个实战例子是在CIFAR-10数据集上训练Vision Transformer(ViT)。通过这个例子，您可以了解MindCV训练的基本流程和调试的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4d9a1d",
   "metadata": {},
   "source": [
    "## 安装\n",
    "\n",
    "首先是安装硬件设备的驱动， 详情请见[GPU设备安装参考](https://www.nvidia.cn/geforce/drivers/)或[昇腾设备安装参考](https://support.huawei.com/enterprise/zh/doc/EDOC1100289994)。\n",
    "\n",
    "然后是安装MindSpore和MindCV。您可以通过学习[MindSpore官网安装教程](https://www.mindspore.cn/install)安装MindSpore最新的版本。安装完成后，可以运行:\n",
    "\n",
    "```\n",
    ">>> import mindspore\n",
    ">>> mindspore.run_check()\n",
    "```\n",
    "如果出现mindspore的版本号，则说明安装成功。\n",
    "\n",
    "\n",
    "MindCV可以通过`pip`安装：\n",
    "```\n",
    "pip install mindcv\n",
    "```\n",
    "\n",
    "也可以通过源代码安装（推荐用这种方式，能够安装最新的版本）：\n",
    "```\n",
    "pip install git+https://github.com/mindspore-lab/mindcv.git\n",
    "```\n",
    "\n",
    "注意当前MindCV要求的MindSpore最低版本为`1.8.1`。\n",
    "如果`import mindcv`没有出现报错，说明MindCV安装成功。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fea7f8",
   "metadata": {},
   "source": [
    "## 关于MindSpore的动态图和静态图模式\n",
    "\n",
    "MindSpore支持动态图和静态图两种模式。在静态图模式下，程序会先编译网络结构，后进行计算。编译器能利用图优化等技术对执行图进行更大程度的优化，从而获得更好的执行性能。而在动态图模式下，程序按照代码的编写顺序执行，在执行正向过程中根据反向传播的原理，动态生成反向执行图。这种模式下，编译器将神经网络中的各个算子逐一下发执行，方便用户编写和调试神经网络模型。详情参考[这里](https://www.mindspore.cn/docs/zh-CN/r2.0/design/dynamic_graph_and_static_graph.html)。\n",
    "\n",
    "MindSpore通过一行简单的代码就能实现静态图和动态图的切换 （由于静态图的语法受限，可以先尝试用动态图模式调试，再转换成静态图模式）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bee788f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore as ms\n",
    "# 静态图模式\n",
    "# ms.set_context(mode=ms.GRAPH_MODE, device_target=\"CPU\")\n",
    "# 动态图模式\n",
    "ms.set_context(mode=ms.PYNATIVE_MODE, device_target=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9366dc29",
   "metadata": {},
   "source": [
    "有了这行代码，接下来我们运行的模型计算将会在对应的静态图或者动态图模式下进行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220991e6",
   "metadata": {},
   "source": [
    "## 定义数据集\n",
    "\n",
    "目前MindCV支持直接下载的数据集有`MNIST`, `CIFAR10`和`CIFAR100`。可以通过下面的代码将`CIFAR10`数据集下载到指定的文件夹：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86b07062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set samples: 50000; test set samples: 10000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from mindcv.data import create_dataset, create_transforms, create_loader\n",
    "\n",
    "cifar10_dir = './cifar-10-batches-bin'  # your dataset path\n",
    "num_classes = 10  # num of classes\n",
    "num_workers = 8  # num of parallel workers\n",
    "\n",
    "# create dataset\n",
    "dataset_train = create_dataset(\n",
    "    name='cifar10', root=cifar10_dir, split='train', shuffle=True, num_parallel_workers=num_workers, download=True\n",
    ")\n",
    "dataset_test = create_dataset(\n",
    "    name='cifar10', root=cifar10_dir, split='test', shuffle=False, num_parallel_workers=num_workers, download=True\n",
    ")\n",
    "print(f\"train set samples: {len(dataset_train)}; test set samples: {len(dataset_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0687b9a",
   "metadata": {},
   "source": [
    "## 定义数据处理函数\n",
    "接下来，我们为training set 和 test set创建data transformations。通常training set数据会经过一些数据增强，例如随机裁剪，随机翻转。例如MindCV中设置的CIFAR10 data transformation 函数：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4728fd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore.dataset import vision\n",
    "def transforms_cifar(resize=224, is_training=True):\n",
    "    \"\"\"Transform operation list when training or evaluating on cifar.\"\"\"\n",
    "    trans = []\n",
    "    if is_training:\n",
    "        trans += [\n",
    "            vision.RandomCrop((32, 32), (4, 4, 4, 4)),\n",
    "            vision.RandomHorizontalFlip(prob=0.5),\n",
    "        ]\n",
    "\n",
    "    trans += [\n",
    "        vision.Resize(resize),\n",
    "        vision.Rescale(1.0 / 255.0, 0.0),\n",
    "        vision.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]),\n",
    "        vision.HWC2CHW(),\n",
    "    ]\n",
    "\n",
    "    return trans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf025fe",
   "metadata": {},
   "source": [
    "另外，`transforms_cifar`这个函数也可以通过下面的方式得到："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f8cb537",
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_train = create_transforms(dataset_name='cifar10', image_resize=224, is_training=True)\n",
    "trans_test = create_transforms(dataset_name='cifar10', image_resize=224, is_training=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993203ef",
   "metadata": {},
   "source": [
    "## 定义数据加载器\n",
    "\n",
    "我们在数据集加载器将对应的dataset 作为参数输入，并设置batch size, num_workers等参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "669889a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train batches: 782 test batches: 157\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_classes = 10\n",
    "num_workers = 8\n",
    "loader_train = create_loader(dataset=dataset_train,\n",
    "                             batch_size=batch_size,\n",
    "                             is_training=True,\n",
    "                             num_classes=num_classes,\n",
    "                             transform=trans_train,\n",
    "                             num_parallel_workers=num_workers)\n",
    "\n",
    "num_batches_train = loader_train.get_dataset_size()\n",
    "\n",
    "loader_test = create_loader(dataset=dataset_test,\n",
    "                             batch_size=batch_size,\n",
    "                             is_training=False,\n",
    "                             num_classes=num_classes,\n",
    "                             transform=trans_test,\n",
    "                             num_parallel_workers=num_workers)\n",
    "\n",
    "num_batches_test = loader_test.get_dataset_size()\n",
    "print(f\"train batches: {num_batches_train} test batches: {num_batches_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336a56b8",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "MindCV 支持许多模型，例如ResNet, ViT等。用户可以通过`mindcv.list_models`查询支持的模型:\n",
    "\n",
    "```\n",
    "import mindcv\n",
    "# 列出所有MindCV支持的模型\n",
    ">>> mindcv.list_models(\"*\")\n",
    "\n",
    "# 列出所有MindCV支持的预训练模型\n",
    ">>> mindcv.list_models(\"*\", pretrained=True)\n",
    "\n",
    "# 列出所有MindCV支持的预训练ViT模型\n",
    ">>> mindcv.list_models(\"vit*\", pretrained=True)\n",
    ">>> ['vit_b_32_224', 'vit_l_16_224', 'vit_l_32_224']\n",
    "```\n",
    "\n",
    "以`vit_b_32_224`为例，创建一个`vit_b_32_224`模型只需要一行代码：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9a5166f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.428.518 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.439.070 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.442.677 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.459.629 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.472.977 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.476.442 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.492.689 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.501.724 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.505.273 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.521.152 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.530.364 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.533.762 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.549.939 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.559.356 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.562.717 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.578.691 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.587.958 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.591.430 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.608.837 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.618.680 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.621.977 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.640.373 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.649.904 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.653.130 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.671.738 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.681.363 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.684.541 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.703.447 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.713.277 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.716.767 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.735.335 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.750.693 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.754.151 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.773.064 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.782.886 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.786.062 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.804.613 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:43.808.650 [mindspore/nn/layer/basic.py:167] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:44.774.104 [mindspore/train/serialization.py:1055] For 'load_param_into_net', 2 parameters in the 'net' are not loaded, because they are not in the 'parameter_dict', please check whether the network structure is consistent when training and loading checkpoint.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:44.775.443 [mindspore/train/serialization.py:1060] head.classifier.weight is not loaded.\n",
      "[WARNING] ME(4060:139963143877568,MainProcess):2023-06-01-09:32:44.776.404 [mindspore/train/serialization.py:1060] head.classifier.bias is not loaded.\n"
     ]
    }
   ],
   "source": [
    "import mindcv\n",
    "network = mindcv.create_model('vit_b_32_224',  num_classes=num_classes, pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b52bb7a",
   "metadata": {},
   "source": [
    "这行代码会在`~/.mindspore/models/`路径下下载一个在`ImageNet-1K`预训练好的模型权重。 在下一次创建该模型时，程序会直接载入下载好的权重。\n",
    "由于`ImageNet-1K`预训练模型的`num_classes`数量与`CIFAR10`数据集的`num_classes`不相等，所以会出现一条`warning message`，提示`classifier`的权重并没有载入。这并不影响我们在`CIFAR10`数据集继续`finetun`e。您也可以选择将`pretrained`参数设为`False`, 这样模型权重会采用随机的初始化值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd779d6",
   "metadata": {},
   "source": [
    "## 定义损失函数\n",
    "\n",
    "使用`create_loss`定义损失函数。 目前`create_loss`支持的损失函数有`CE` (cross entropy) 和`BCE` (binary cross entropy) 。未来还会继续支持更多常用的损失函数。\n",
    "现在我们使用cross entropy作为损失函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5db28f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindcv.loss import create_loss\n",
    "\n",
    "loss = create_loss(name='CE')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d18592",
   "metadata": {},
   "source": [
    "## 定义学习率的调整策略\n",
    "使用`create_scheduler`定义学习率的调整策略。`create_scheduler`支持包括`constant`, `step_decay`, `cosine_decay`在内的多种策略，详情请见[这里](https://mindcv.readthedocs.io/en/latest/api/mindcv.scheduler.html#mindcv.scheduler.create_scheduler)。在本例子中，我们使用固定的学习率$0.0001$。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0b32f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindcv.scheduler import create_scheduler\n",
    "# learning rate scheduler\n",
    "lr_scheduler = create_scheduler(steps_per_epoch=num_batches_train,\n",
    "                                scheduler='constant',\n",
    "                                lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ebc84e",
   "metadata": {},
   "source": [
    "## 定义优化器\n",
    "\n",
    "使用`create_optimizer`定义损失函数。 `create_optimizer`支持各种主流的优化器，例如`adam`, `adamw`, `sgd`等。优化器的选择可以通过`opt`这个参数传递。另外`create_optimizer` 的第一个参数`params`既可以是一个由`Parameter`组成的列表，例如`network.trainable_params()`,也可以是一个字典。例如：\n",
    "```\n",
    "from mindspore import nn\n",
    "from mindcv.optim import create_optimizer\n",
    "net = Net()\n",
    "\n",
    "# Convolutional parameter\n",
    "conv_params = list(filter(lambda x: 'conv' in x.name, net.trainable_params()))\n",
    "# Non-convolutional parameter\n",
    "no_conv_params = list(filter(lambda x: 'conv' not in x.name, net.trainable_params()))\n",
    "\n",
    "# Fixed learning rate\n",
    "fix_lr = 0.01\n",
    "\n",
    "# Computation of Learning Rate Based on Polynomial Decay Function\n",
    "polynomial_decay_lr = nn.PolynomialDecayLR(learning_rate=0.1,      # Initial learning rate\n",
    "                                           end_learning_rate=0.01, # Final the learning rate\n",
    "                                           decay_steps=4,          #Number of decay steps\n",
    "                                           power=0.5)              # Polynomial power\n",
    "\n",
    "# The convolutional parameter uses a fixed learning rate of 0.001, and the weight decay is 0.01.\n",
    "# The non-convolutional parameter uses a dynamic learning rate, and the weight decay is 0.0.\n",
    "group_params = [{'params': conv_params, 'weight_decay': 0.01, 'lr': fix_lr},\n",
    "                {'params': no_conv_params, 'lr': polynomial_decay_lr}]\n",
    "optim = create_optimizer(group_params, \"adam\", lr=0.1, momentum=0.9, weight_decay=0.0)\n",
    "\n",
    "```\n",
    "通过字典可以针对模型的不同部分设置不同学习率等。\n",
    "\n",
    "\n",
    "在本例子中，我们选择`adam`优化器对模型全部的trainable权重进行更新。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a649257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindcv.optim import create_optimizer\n",
    "\n",
    "# create optimizer\n",
    "opt = create_optimizer(network.trainable_params(), opt='adam', lr=lr_scheduler)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8190ea",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "[mindspore.Model](https://mindspore.cn/docs/zh-CN/r1.8/api_python/mindspore/mindspore.Model.html)提供了非常丰富的接口。用户可以传入与训练相关的各种参数，例如损失函数，评价函数， 混合精度。\n",
    "\n",
    "我们将前面定义好的损失函数，学习率策略，优化器和模型作为参数传递给`Model`：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27c259c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import Model\n",
    "\n",
    "# Encapsulates examples that can be trained or inferred\n",
    "model = Model(network, loss_fn=loss, optimizer=opt, metrics={'accuracy'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d098d87",
   "metadata": {},
   "source": [
    "[mindspore.Model.train](https://mindspore.cn/docs/zh-CN/r1.8/api_python/mindspore/mindspore.Model.html#mindspore.Model.train) 能够开启训练过程，并且根据传入函数的参数，设置训练所需要的轮次(epochs），训练数据集，回调对象（callbacks, 在训练中途进行loss printing, checkpoint saving 等操作）。另外，要理解`dataset_sink_mode`参数可以参考这里的[资料](https://www.mindspore.cn/tutorials/experts/zh-CN/master/optimize/execution_opt.html)。简单理解就是开启模型计算和数据载入的并行模式。在`PYNATIVE`模式下，`dataset_sink_mode`默认是关闭的。关闭`dataset_sink_mode`有助于调试， 开启`dataset_sink_mode`通常能够获得更快的训练速度。\n",
    "\n",
    "回调对象可以丰富我们对训练过程的监控。 这里我们设置了每`num_batches_train//5`个steps就输出一次损失函数的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4d222e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 156, loss is 2.1286168098449707\n",
      "epoch: 1 step: 312, loss is 1.1996170282363892\n",
      "epoch: 1 step: 468, loss is 0.471361368894577\n",
      "epoch: 1 step: 624, loss is 0.2324763387441635\n",
      "epoch: 1 step: 780, loss is 0.20593130588531494\n",
      "Train epoch time: 1864830.944 ms, per step time: 2384.694 ms\n",
      "epoch: 2 step: 154, loss is 0.14241477847099304\n",
      "epoch: 2 step: 310, loss is 0.13663174211978912\n",
      "epoch: 2 step: 466, loss is 0.16266559064388275\n",
      "epoch: 2 step: 622, loss is 0.12892954051494598\n",
      "epoch: 2 step: 778, loss is 0.11221715807914734\n",
      "Train epoch time: 1768254.883 ms, per step time: 2261.196 ms\n",
      "epoch: 3 step: 152, loss is 0.19256466627120972\n",
      "epoch: 3 step: 308, loss is 0.23028206825256348\n",
      "epoch: 3 step: 464, loss is 0.1350468248128891\n",
      "epoch: 3 step: 620, loss is 0.026827018707990646\n",
      "epoch: 3 step: 776, loss is 0.1309332251548767\n",
      "Train epoch time: 1788569.746 ms, per step time: 2287.174 ms\n",
      "epoch: 4 step: 150, loss is 0.06645049899816513\n",
      "epoch: 4 step: 306, loss is 0.12062713503837585\n",
      "epoch: 4 step: 462, loss is 0.06675134599208832\n",
      "epoch: 4 step: 618, loss is 0.07662423700094223\n",
      "epoch: 4 step: 774, loss is 0.08617927134037018\n",
      "Train epoch time: 1762236.646 ms, per step time: 2253.500 ms\n",
      "epoch: 5 step: 148, loss is 0.0892096608877182\n",
      "epoch: 5 step: 304, loss is 0.08265252411365509\n",
      "epoch: 5 step: 460, loss is 0.06244994327425957\n",
      "epoch: 5 step: 616, loss is 0.11957444250583649\n",
      "epoch: 5 step: 772, loss is 0.03380810469388962\n",
      "Train epoch time: 1807736.172 ms, per step time: 2311.683 ms\n"
     ]
    }
   ],
   "source": [
    "from mindspore import LossMonitor, TimeMonitor, CheckpointConfig, ModelCheckpoint\n",
    "\n",
    "# Set the callback function for saving network parameters during training.\n",
    "ckpt_save_dir = './ckpt'\n",
    "ckpt_config = CheckpointConfig(save_checkpoint_steps=num_batches_train)\n",
    "ckpt_cb = ModelCheckpoint(prefix='vit-cifar10',\n",
    "                          directory=ckpt_save_dir,\n",
    "                          config=ckpt_config)\n",
    "loss_monitor = LossMonitor(num_batches_train//5)\n",
    "time_monitor = TimeMonitor(num_batches_train//5)\n",
    "num_epochs = 5\n",
    "model.train(num_epochs, loader_train, callbacks=[loss_monitor,time_monitor, ckpt_cb], dataset_sink_mode=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3fa8cd",
   "metadata": {},
   "source": [
    "## 测试\n",
    "\n",
    "训练完成后，我们对模型的准确度进行测试，结果如下：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bd1eebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9719}\n"
     ]
    }
   ],
   "source": [
    "acc = model.eval(loader_test, dataset_sink_mode=False)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
